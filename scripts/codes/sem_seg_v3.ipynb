{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom dataset for loading the original images and using final output images as masks\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(os.listdir(self.image_dir))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.listdir(self.image_dir)[idx]\n",
    "        image_path = os.path.join(self.image_dir, img_name)\n",
    "        mask_path = os.path.join(self.mask_dir, img_name)\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        mask = Image.open(mask_path).convert(\"RGB\")  # Assuming final output images are RGB\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            mask = self.transform(mask)\n",
    "\n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# U-Net implementation for semantic segmentation\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder_conv1 = self.conv_block(in_channels, 64)\n",
    "        self.encoder_conv2 = self.conv_block(64, 128)\n",
    "        self.encoder_conv3 = self.conv_block(128, 256)\n",
    "        self.encoder_conv4 = self.conv_block(256, 512)\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder_upconv3 = self.conv_transpose(512, 256)\n",
    "        self.decoder_upconv2 = self.conv_transpose(256, 128)\n",
    "        self.decoder_upconv1 = self.conv_transpose(128, 64)\n",
    "        \n",
    "        self.decoder_conv3 = self.conv_block(512, 256)\n",
    "        self.decoder_conv2 = self.conv_block(256, 128)\n",
    "        self.decoder_conv1 = self.conv_block(128, out_channels)\n",
    "        \n",
    "    def conv_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    \n",
    "    def conv_transpose(self, in_channels, out_channels):\n",
    "        return nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        enc1 = self.encoder_conv1(x)\n",
    "        enc2 = self.encoder_conv2(nn.MaxPool2d(2)(enc1))\n",
    "        enc3 = self.encoder_conv3(nn.MaxPool2d(2)(enc2))\n",
    "        enc4 = self.encoder_conv4(nn.MaxPool2d(2)(enc3))\n",
    "        \n",
    "        # Decoder\n",
    "        dec3 = self.decoder_upconv3(enc4)\n",
    "        dec3 = torch.cat([dec3, enc3], dim=1)\n",
    "        dec3 = self.decoder_conv3(dec3)\n",
    "        \n",
    "        dec2 = self.decoder_upconv2(dec3)\n",
    "        dec2 = torch.cat([dec2, enc2], dim=1)\n",
    "        dec2 = self.decoder_conv2(dec2)\n",
    "        \n",
    "        dec1 = self.decoder_upconv1(dec2)\n",
    "        dec1 = torch.cat([dec1, enc1], dim=1)\n",
    "        dec1 = self.decoder_conv1(dec1)\n",
    "        \n",
    "        return dec1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the paths to your original images and semantic segmented images\n",
    "image_dir = \"/home/tamoghna/catkin_ws/src/dynamix/scripts/input/dataset/1/\"\n",
    "mask_dir = \"/home/tamoghna/catkin_ws/src/dynamix/scripts/input/dataset/2/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the transformation to apply to the images\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor()\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataset and data loader\n",
    "dataset = CustomDataset(image_dir, mask_dir, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "in_channels = 3  # Number of input channels (RGB)\n",
    "out_channels = 5  # Number of output classes (based on your dataset)\n",
    "model = UNet(in_channels, out_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move the model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: : 7440it [27:46,  4.46it/s, loss=0.351]                        \n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 10\n",
    "progress_bar = tqdm(total=len(dataloader))\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for images, masks in dataloader:\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # Convert RGB mask images to class indices\n",
    "        masks = torch.argmax(masks, dim=1).long()\n",
    "\n",
    "        loss = criterion(outputs, masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        progress_bar.set_description(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        progress_bar.set_postfix(loss=running_loss / len(dataloader))\n",
    "        progress_bar.update(1)\n",
    "\n",
    "\n",
    "progress_bar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "torch.save(model.state_dict(), \"seggs.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UNet(\n",
       "  (encoder_conv1): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "  )\n",
       "  (encoder_conv2): Sequential(\n",
       "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "  )\n",
       "  (encoder_conv3): Sequential(\n",
       "    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "  )\n",
       "  (encoder_conv4): Sequential(\n",
       "    (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "  )\n",
       "  (decoder_upconv3): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))\n",
       "  (decoder_upconv2): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
       "  (decoder_upconv1): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
       "  (decoder_conv3): Sequential(\n",
       "    (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "  )\n",
       "  (decoder_conv2): Sequential(\n",
       "    (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "  )\n",
       "  (decoder_conv1): Sequential(\n",
       "    (0): Conv2d(128, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(5, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = UNet(in_channels, out_channels)  # Initialize the UNet model\n",
    "model.load_state_dict(torch.load())  # Load the saved model weights\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\"/home/tamoghna/catkin_ws/src/dynamix/scripts/seggs.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(\"/home/tamoghna/catkin_ws/src/dynamix/scripts/test.jpg\").convert(\"RGB\")  # Load the input image\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "image = transform(image).unsqueeze(0)  # Apply transformations and add a batch dimension\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m----> 2\u001b[0m     output \u001b[39m=\u001b[39m model(image\u001b[39m.\u001b[39;49mto(device))  \u001b[39m# Pass the preprocessed image through the model\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[3], line 34\u001b[0m, in \u001b[0;36mUNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m     33\u001b[0m     \u001b[39m# Encoder\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m     enc1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder_conv1(x)\n\u001b[1;32m     35\u001b[0m     enc2 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder_conv2(nn\u001b[39m.\u001b[39mMaxPool2d(\u001b[39m2\u001b[39m)(enc1))\n\u001b[1;32m     36\u001b[0m     enc3 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder_conv3(nn\u001b[39m.\u001b[39mMaxPool2d(\u001b[39m2\u001b[39m)(enc2))\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    output = model(image.to(device))  # Pass the preprocessed image through the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = torch.argmax(output, dim=1)  # Convert the output to class indices\n",
    "output_image = output.squeeze(0).cpu().numpy()  # Remove the batch dimension and move to CPU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the saved model weights\n",
    "model.load_state_dict(torch.load(\"/home/tamoghna/catkin_ws/src/dynamix/scripts/seggs.pth\"))\n",
    "\n",
    "# Move the model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Preprocess the input image\n",
    "input_image = Image.open(\"/home/tamoghna/catkin_ws/src/dynamix/scripts/test3.jpeg\").convert(\"RGB\")\n",
    "input_image = transform(input_image).unsqueeze(0).to(device)\n",
    "\n",
    "# Perform inference using the model\n",
    "with torch.no_grad():\n",
    "    output = model(input_image)\n",
    "\n",
    "# Post-process the output\n",
    "output = torch.argmax(output, dim=1)\n",
    "output_image = output.squeeze(0).cpu().numpy()\n",
    "\n",
    "# Visualize the segmentation result\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cmap = plt.get_cmap(\"jet\", out_channels)  # Assuming out_channels is the number of classes\n",
    "plt.imshow(output_image, cmap=cmap)\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
